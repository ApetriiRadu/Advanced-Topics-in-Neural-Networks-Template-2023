{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ApetriiRadu/Advanced-Topics-in-Neural-Networks-Template-2023/blob/main/Lab05/Solution/Lab05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rCnYEMJCLQt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from multiprocessing import freeze_support\n",
        "\n",
        "import torch\n",
        "import wandb\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import v2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sam import SAM\n",
        "\n",
        "\n",
        "def get_default_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.device('mos')\n",
        "    return torch.device('cpu')\n",
        "\n",
        "\n",
        "class CachedDataset(Dataset):\n",
        "    def __init__(self, dataset, cache=True):\n",
        "        if cache:\n",
        "            dataset = tuple([x for x in dataset])\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.dataset[i]\n",
        "\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = torch.nn.Linear(hidden_size, output_size)\n",
        "        self.relu = torch.nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "        # x = self.fc1(x)\n",
        "        # x = self.relu(x)\n",
        "        # x = self.fc2(x)\n",
        "        # return x\n",
        "\n",
        "class SAM(torch.optim.Optimizer):\n",
        "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
        "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
        "\n",
        "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
        "        super(SAM, self).__init__(params, defaults)\n",
        "\n",
        "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
        "        self.param_groups = self.base_optimizer.param_groups\n",
        "        self.defaults.update(self.base_optimizer.defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def first_step(self, zero_grad=False):\n",
        "        grad_norm = self._grad_norm()\n",
        "        for group in self.param_groups:\n",
        "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                self.state[p][\"old_p\"] = p.data.clone()\n",
        "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
        "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def second_step(self, zero_grad=False):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
        "\n",
        "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
        "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
        "\n",
        "        self.first_step(zero_grad=True)\n",
        "        closure()\n",
        "        self.second_step()\n",
        "\n",
        "    def _grad_norm(self):\n",
        "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
        "        norm = torch.norm(\n",
        "                    torch.stack([\n",
        "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
        "                        for group in self.param_groups for p in group[\"params\"]\n",
        "                        if p.grad is not None\n",
        "                    ]),\n",
        "                    p=2\n",
        "               )\n",
        "        return norm\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        super().load_state_dict(state_dict)\n",
        "        self.base_optimizer.param_groups = self.param_groups\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    fp_plus_fn = torch.logical_not(output == labels).sum().item()\n",
        "    all_elements = len(output)\n",
        "    return (all_elements - fp_plus_fn) / all_elements\n",
        "\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "\n",
        "    all_outputs = []\n",
        "    all_labels = []\n",
        "\n",
        "    batches_loss = []\n",
        "\n",
        "    sam = wandb.config.optimizer == 4 or wandb.config.optimizer == 5\n",
        "\n",
        "    for data, labels in train_loader:\n",
        "        data = data.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        output = model(data)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "\n",
        "        def closure():\n",
        "            output = model(data)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            return loss\n",
        "\n",
        "        if sam:\n",
        "            optimizer.step(closure)\n",
        "        else:\n",
        "            optimizer.step()\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        output = output.softmax(dim=1).detach().cpu().squeeze()\n",
        "        labels = labels.cpu().squeeze()\n",
        "        all_outputs.append(output)\n",
        "        all_labels.append(labels)\n",
        "\n",
        "        batches_loss.append(loss.item())\n",
        "\n",
        "    all_outputs = torch.cat(all_outputs).argmax(dim=1)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    return round(accuracy(all_outputs, all_labels), 4), batches_loss\n",
        "\n",
        "\n",
        "def val(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "\n",
        "    all_outputs = []\n",
        "    all_labels = []\n",
        "\n",
        "    val_loss = 0\n",
        "\n",
        "    for data, labels in val_loader:\n",
        "        data = data.to(device, non_blocking=True)\n",
        "        labels_cuda = labels.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(data)\n",
        "\n",
        "        val_loss += criterion(output, labels_cuda).item()\n",
        "\n",
        "        output = output.softmax(dim=1).cpu().squeeze()\n",
        "        labels = labels.squeeze()\n",
        "        all_outputs.append(output)\n",
        "        all_labels.append(labels)\n",
        "\n",
        "    all_outputs = torch.cat(all_outputs).argmax(dim=1)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    return round(accuracy(all_outputs, all_labels), 4), val_loss\n",
        "\n",
        "\n",
        "def do_epoch(model, train_loader, val_loader, criterion, optimizer, device):\n",
        "    acc, batches_loss = train(model, train_loader, criterion, optimizer, device)\n",
        "    acc_val, val_loss = val(model, val_loader, criterion, device)\n",
        "    # torch.cuda.empty_cache()\n",
        "    return acc, acc_val, batches_loss, val_loss\n",
        "\n",
        "\n",
        "def get_model_norm(model):\n",
        "    norm = 0.0\n",
        "    for param in model.parameters():\n",
        "        norm += torch.norm(param)\n",
        "    return norm\n",
        "\n",
        "\n",
        "def main(device=get_default_device(), config=None):\n",
        "    transforms = [\n",
        "        v2.ToImage(),\n",
        "        v2.ToDtype(torch.float32, scale=True),\n",
        "        v2.Resize((28, 28), antialias=True),\n",
        "        v2.Grayscale(),\n",
        "        torch.flatten,\n",
        "    ]\n",
        "\n",
        "    data_path = '../data'\n",
        "    train_dataset = CIFAR10(root=data_path, train=True, transform=v2.Compose(transforms), download=True)\n",
        "    val_dataset = CIFAR10(root=data_path, train=False, transform=v2.Compose(transforms), download=True)\n",
        "    train_dataset = CachedDataset(train_dataset)\n",
        "    val_dataset = CachedDataset(val_dataset)\n",
        "\n",
        "    model = MLP(784, 100, 10)\n",
        "    model = model.to(device)\n",
        "\n",
        "    epochs = 60\n",
        "    val_batch_size = 500\n",
        "    num_workers = 2\n",
        "    persistent_workers = (num_workers != 0)\n",
        "    pin_memory = device.type == 'cuda'\n",
        "\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "        train_loader = DataLoader(train_dataset, shuffle=True, pin_memory=pin_memory, num_workers=num_workers,\n",
        "                            batch_size=config.batch_size, drop_last=True, persistent_workers=persistent_workers)\n",
        "        val_loader = DataLoader(val_dataset, shuffle=False, pin_memory=True, num_workers=0, batch_size=val_batch_size,\n",
        "                                drop_last=False)\n",
        "        if config.optimizer == 0:\n",
        "            optimizer = torch.optim.SGD(model.parameters(), lr=config.learning_rate)\n",
        "        elif config.optimizer == 1:\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "        elif config.optimizer == 2:\n",
        "            optimizer = torch.optim.RMSprop(model.parameters(), lr=config.learning_rate)\n",
        "        elif config.optimizer == 3:\n",
        "            optimizer = torch.optim.Adagrad(model.parameters(), lr=config.learning_rate)\n",
        "        elif config.optimizer == 4:\n",
        "            base_optimizer = torch.optim.SGD\n",
        "            optimizer = SAM(model.parameters(), base_optimizer, lr=config.learning_rate, momentum=0.9)\n",
        "        elif config.optimizer == 5:\n",
        "            base_optimizer = torch.optim.SGD\n",
        "            optimizer = SAM(model.parameters(), base_optimizer, rho = 2.0, adaptive=True, lr=config.learning_rate, momentum=0.9)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        writer = SummaryWriter()\n",
        "        tbar = tqdm(tuple(range(epochs)))\n",
        "\n",
        "        optimizer_name = (type (optimizer).__name__)\n",
        "        writer.add_scalar(\"Params/Learning_rate\", config.learning_rate)\n",
        "        writer.add_scalar(\"Params/Batch_size\", config.batch_size)\n",
        "        writer.add_scalar(\"Params/Optimizer\", config.optimizer)\n",
        "\n",
        "        for epoch in tbar:\n",
        "            acc, acc_val, batches_loss, val_loss = do_epoch(model, train_loader, val_loader, criterion, optimizer, device)\n",
        "            epoch_loss = sum(batches_loss) / len(batches_loss)\n",
        "            tbar.set_postfix_str(f\"Optimizer: {optimizer_name}, Acc: {acc}, Acc_val: {acc_val}\")\n",
        "            writer.add_scalar(\"Train/Accuracy\", acc, epoch)\n",
        "            writer.add_scalar(\"Val/Accuracy\", acc_val, epoch)\n",
        "            writer.add_scalar(\"Train/Loss\", epoch_loss, epoch)\n",
        "            writer.add_scalar(\"Val/Loss\", val_loss, epoch)\n",
        "            writer.add_scalar(\"Model/Norm\", get_model_norm(model), epoch)\n",
        "            for i in range(0, len(batches_loss)):\n",
        "                writer.add_scalar(\"Train/Batch\", batches_loss[i], i)\n",
        "\n",
        "            wandb.log({\"epoch\": epoch, \"accuracy\": acc_val})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    wandb.login()\n",
        "    sweep_config = {\n",
        "    'method': 'random'\n",
        "    }\n",
        "\n",
        "    parameters_dict = {\n",
        "    'optimizer': {\n",
        "        'values': [0, 1, 2, 3, 4, 5]\n",
        "        },\n",
        "    'learning_rate': {\n",
        "        'distribution': 'uniform',\n",
        "        'min': 0,\n",
        "        'max': 0.05\n",
        "      },\n",
        "    'batch_size': {\n",
        "        'distribution': 'q_log_uniform_values',\n",
        "        'q': 8,\n",
        "        'min': 64,\n",
        "        'max': 256,\n",
        "      }\n",
        "    }\n",
        "\n",
        "    sweep_config['parameters'] = parameters_dict\n",
        "    sweep_id = wandb.sweep(sweep_config, project=\"Lab05\")\n",
        "    freeze_support()\n",
        "    wandb.agent(sweep_id, main, count=6)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}